{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Lake (Delta Lake) Analysis with PySpark and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook demonstrates the process of performing Exploratory Data Analysis (EDA) on data stored in Delta Lake format using PySpark. The script analyzes user-item interaction data, providing insights into user behavior, product popularity, and seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n\nBefore running, ensure you have `PySpark`, `delta-spark`, `matplotlib`, `seaborn`, and `pandas` installed. JDK and Hadoop are also required.\n\n*   `pyspark` (pip install pyspark)\n*   `delta-spark` (will be downloaded by Spark)\n*   `matplotlib` (pip install matplotlib)\n*   `seaborn` (pip install seaborn)\n*   `pandas` (pip install pandas)\n\nHadoop and PySpark environment variables are configured for correct Spark operation on a local machine. **Be sure to change the HADOOP path to your actual path!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set environment variables for HADOOP and PySpark\n",
    "# Make sure the paths correspond to your HADOOP installation\n",
    "os.environ['HADOOP_HOME'] = r'C:\\HADOOP' # <--- CHANGE THIS PATH TO YOURS\n",
    "os.environ['PATH'] = os.environ['PATH'] + ';' + os.environ['HADOOP_HOME'] + '\\\\bin'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# PySpark and Delta Lake imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import(\n",
    "    col,\n",
    "    count,\n",
    "    countDistinct,\n",
    "    when,\n",
    "    sum as _sum,\n",
    "    avg,\n",
    "    hour,\n",
    "    dayofweek,\n",
    "    month,\n",
    "    date_format,\n",
    "    datediff,\n",
    "    lit,\n",
    "    min as _min,\n",
    "    max as _max\n",
    ")\n",
    "from pyspark.sql.types import (IntegerType, FloatType, TimestampType)\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n\n### `create_spark_session`\n\nFunction to initialize a SparkSession with the necessary configurations for working with Delta Lake. It configures Spark SQL extensions for Delta and includes the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Creates and returns a SparkSession with Delta Lake support.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): Name of the Spark application.\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: Initialized SparkSession object.\n",
    "    \"\"\"\n",
    "    print(\"Initializing Spark Session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark Session successfully initialized.\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_spark_df`\n\nUtility function for plotting from a Spark DataFrame. It converts the Spark DataFrame to a Pandas DataFrame for use with `seaborn` and `matplotlib`.\n\n**Important Note:** The `.toPandas()` method loads the entire Spark DataFrame into the driver's memory, which can lead to performance issues and OutOfMemory errors when working with very large datasets. Use it cautiously or for aggregated/filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spark_df(spark_df, x_col, y_col, title, kind='bar', figsize=(12,6)):\n",
    "    \"\"\"\n",
    "    Plots a graph from a Spark DataFrame, first converting it to a Pandas DataFrame.\n",
    "    Supports bar and line plots.\n",
    "    \n",
    "    Args:\n",
    "        spark_df (DataFrame): Spark DataFrame to visualize.\n",
    "        x_col (str): Column name for the X-axis.\n",
    "        y_col (str): Column name for the Y-axis.\n",
    "        title (str): Title of the plot.\n",
    "        kind (str): Type of plot ('bar' or 'line').\n",
    "        figsize (tuple): Figure size for matplotlib.\n",
    "    \"\"\"\n",
    "    print(f\"Plotting graph: {title}...\")\n",
    "    try:\n",
    "        # Convert Spark DataFrame to Pandas DataFrame\n",
    "        pd_df = spark_df.toPandas()\n",
    "        plt.figure(figsize=figsize)\n",
    "        if kind == 'bar':\n",
    "            sns.barplot(data=pd_df.sort_values(y_col, ascending=False), x=x_col, y=y_col)\n",
    "        elif kind == 'line':\n",
    "            sns.lineplot(data=pd_df.sort_values(x_col), x=x_col, y=y_col, marker='o')\n",
    "        else:\n",
    "            print(f\"Unknown plot type: {kind}\")\n",
    "            return\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting graph {title}: {e}\")\n",
    "        print(f\"Perhaps the DataFrame is too large for .toPandas() or another error occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main EDA Function: `run_eda`\n\nThis function performs a comprehensive exploratory data analysis of data loaded from Delta Lake. It includes:\n\n*   Schema overview and data preview.\n*   NULL value check.\n*   Analysis of interaction type distribution.\n*   User analysis (loyalty, cities, activity, 'cold' users).\n*   Product analysis (popular categories/brands, 'cold' products).\n*   Seasonality analysis (activity by hour, day of week, day).\n*   Funnel and correlation analysis (relation of search queries to purchases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eda(spark:SparkSession, marth_path: str):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis (EDA) for the specified Delta table.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Active SparkSession.\n",
    "        marth_path (str): Path to the Delta table.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting EDA for Delta table: {marth_path}\")\n",
    "    if not DeltaTable.isDeltaTable(spark, marth_path):\n",
    "        print(f\"Error: Delta table at path {marth_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading data from Data Lake...\")\n",
    "    df = spark.read.format(\"delta\").load(marth_path)\n",
    "    df.cache() # Cache the DataFrame for improved performance during multiple operations\n",
    "    total_records = df.count()\n",
    "    print(f\"Loaded {total_records} records.\")\n",
    "\n",
    "    if total_records == 0:\n",
    "        print(\"Table is empty. EDA cannot be performed.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n### Data Overview\")\n",
    "    df.printSchema()\n",
    "    print(\"\\nSample Data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    print(\"\\nKey Statistics (for numeric columns):\")\n",
    "    df.select(\"age\", \"quantity\", \"price_at_interaction\", \"product_current_price\").describe().show()\n",
    "\n",
    "    print(\"\\n### Check for NULL values\")\n",
    "    null_counts = []\n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            null_counts.append((column, null_count, (null_count / total_records) * 100))\n",
    "    if null_counts:\n",
    "        print(\"NULL values found:\")\n",
    "        for col_name, count, percentage in null_counts:\n",
    "            print(f\" - {col_name}: {count} records ({percentage:.2f}%)\") \n",
    "    else:\n",
    "        print(\"NULL values not found (except expected ones). DataFrame is considered ready for analysis.\")\n",
    "    \n",
    "    print(\"\\n### Interaction Analysis\")\n",
    "    interaction_counts = df.groupBy(\"interaction_type\").count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    print(\"\\nDistribution of Interaction Types:\")\n",
    "    interaction_counts.show()\n",
    "    plot_spark_df(interaction_counts, \"interaction_type\", \"count\", \"Distribution of Interaction Types\")\n",
    "\n",
    "    print(\"\\n### User Analysis\")\n",
    "    user_count = df.select(\"user_id\").distinct().count()\n",
    "    print(f\"Total unique users: {user_count}\")\n",
    "\n",
    "    loyalty_dist = df.select(\"user_id\", \"loyalty_tier\").distinct() \\\n",
    "    .groupBy(\"loyalty_tier\").count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    print(\"\\nDistribution of users by loyalty tier:\")\n",
    "    loyalty_dist.show()\n",
    "    plot_spark_df(loyalty_dist, \"loyalty_tier\", \"count\", \"Distribution of Users by Loyalty Tier\")\n",
    "\n",
    "    city_dist = df.select(\"user_id\", \"city\").distinct() \\\n",
    "    .groupBy(\"city\").count() \\\n",
    "    .orderBy(\"count\", ascending=False) \n",
    "    print(\"\\nDistribution of users by city:\")\n",
    "    city_dist.show(10)\n",
    "    plot_spark_df(city_dist.limit(10), \"city\", \"count\", \"TOP-10 Cities by User Count\")\n",
    "    \n",
    "    user_activity = df.groupBy(\"user_id\").count().withColumnRenamed(\"count\", \"interaction_count\")\n",
    "    print(\"\\nUser Activity Statistics:\")\n",
    "    user_activity.select(\n",
    "        avg(\"interaction_count\").alias(\"avg_interactions\"),\n",
    "        _min(\"interaction_count\").alias(\"min_interactions\"),\n",
    "        _max(\"interaction_count\").alias(\"max_interactions\")\n",
    "    ).show()\n",
    "\n",
    "    COLD_START_TRESHOLD = 10 # Threshold for defining 'cold' users/products\n",
    "    cold_users_count = user_activity.filter(col(\"interaction_count\") <= COLD_START_TRESHOLD).count()\n",
    "    print(f\"\\nNumber of 'cold' users (< {COLD_START_TRESHOLD} interactions): {cold_users_count} ({(cold_users_count / user_count) * 100:.2f}%) or {cold_users_count} users\")\n",
    "\n",
    "    print(\"\\n### Product Analysis\")\n",
    "    product_count = df.select(\"item_id\").distinct().count()\n",
    "    print(f\"Total unique products: {product_count}\")\n",
    "\n",
    "    top_categories = df.filter(col(\"interaction_type\") == \"purchase\") \\\n",
    "                       .groupBy(\"category\").count().orderBy(\"count\", ascending=False).limit(10)\n",
    "    \n",
    "    print(\"\\nTOP-10 Categories by Purchases:\")\n",
    "    top_categories.show()\n",
    "    plot_spark_df(top_categories, \"category\", \"count\", \"TOP-10 Categories by Purchases\")\n",
    "\n",
    "    top_brands = df.filter(col(\"interaction_type\") == \"purchase\") \\\n",
    "                   .groupBy(\"brand\").count().orderBy(\"count\", ascending=False).limit(10)\n",
    "    print(\"\\nTOP-10 Brands by Purchases:\")\n",
    "    top_brands.show()\n",
    "    plot_spark_df(top_brands, \"brand\", \"count\", \"TOP-10 Brands by Purchases\")\n",
    "\n",
    "    item_activity = df.groupBy(\"item_id\").count().withColumnRenamed(\"count\", \"interaction_count\")\n",
    "    cold_items_count = item_activity.filter(col(\"interaction_count\") <= COLD_START_TRESHOLD).count()\n",
    "    print(f\"\\nNumber of 'cold' products (< {COLD_START_TRESHOLD} interactions): {cold_items_count} ({(cold_items_count / product_count) * 100 :.2f}%) or {cold_items_count} products\")\n",
    "\n",
    "    cold_items_df = item_activity.filter(col(\"interaction_count\") <= COLD_START_TRESHOLD) \\\n",
    "                                 .orderBy(\"interaction_count\", ascending=True) \\\n",
    "                                 .select(\"item_id\", \"interaction_count\")\n",
    "    \n",
    "    if cold_items_count > 0:\n",
    "        print(f\"\\nList of 'cold' products (TOP-20 least popular):\")\n",
    "        cold_items_df.show(20, truncate=False) \n",
    "    else:\n",
    "        print(f\"\\nNo 'cold' products to display (all products have more than {COLD_START_TRESHOLD} interactions).\")\n",
    "\n",
    "    print(\"\\n### Seasonality Analysis\")\n",
    "    df_ts = df.filter(col(\"timestamp\").isNotNull()) # Filter records with missing timestamps\n",
    "\n",
    "    hourly_activity = df_ts.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "                         .groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "    print(\"\\nActivity by Hour (UTC):\")\n",
    "    hourly_activity.show(24)\n",
    "    plot_spark_df(hourly_activity, \"hour\", \"count\", \"Activity by Hour (UTC)\", kind='line')\n",
    "\n",
    "    dow_activity = df_ts.withColumn(\"dow\", dayofweek(col(\"timestamp\"))) \\\n",
    "    .groupBy(\"dow\").count() \\\n",
    "    .orderBy(\"dow\")\n",
    "\n",
    "    print(\"\\nActivity by Day of Week (1=Sun, 7=Sat):\")\n",
    "    dow_activity.show()\n",
    "    plot_spark_df(dow_activity, \"dow\", \"count\", \"Activity by Day of Week\", kind='line')\n",
    "\n",
    "    daily_activity = df_ts.withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "                        .groupBy(\"date\").count() \\\n",
    "                        .orderBy(\"date\")\n",
    "    \n",
    "    print(\"\\nActivity by Day (first 10 days):\")\n",
    "    daily_activity.show(10)\n",
    "\n",
    "    plot_spark_df(daily_activity.limit(30), \"date\", \"count\", \"Activity by Day (up to 30 days)\", kind='line', figsize=(15,6))\n",
    "\n",
    "    print(\"\\n### Funnel and Correlation Analysis\")\n",
    "    funnel_df = df.groupBy(\"interaction_type\").agg(countDistinct(\"user_id\").alias(\"unique_users\"))\n",
    "    print(\"\\nApproximate Funnel (unique users):\")\n",
    "    funnel_df.orderBy(\"unique_users\", ascending=False).show()\n",
    "\n",
    "    purchasing_users = df.filter(col(\"interaction_type\") == \"purchase\") \\\n",
    "    .select(\"user_id\").distinct()\n",
    "\n",
    "    purchase_related_interactions = df.join(purchasing_users, on=\"user_id\", how=\"inner\") \\\n",
    "                                     .groupBy(\"interaction_type\").count().orderBy(\"count\", ascending=False)\n",
    "    \n",
    "    print(\"\\nDistribution of interactions for users who made a purchase:\")\n",
    "    purchase_related_interactions.show()\n",
    "    plot_spark_df(purchase_related_interactions, \"interaction_type\", \"count\", \"Interactions for Purchasers\")\n",
    "\n",
    "    searches = df.filter(col(\"interaction_type\") == \"search\") \\\n",
    "                                     .select(\"user_id\", \"search_query\").distinct()\n",
    "    \n",
    "    search_to_purchase = searches.join(purchasing_users, on=\"user_id\", how=\"inner\") \\\n",
    "                                     .groupBy(\"search_query\").count().orderBy(\"count\", ascending=False).limit(15)\n",
    "    \n",
    "    print(\"\\nTOP-15 Search Queries from Users who Made a Purchase:\")\n",
    "    search_to_purchase.show(truncate=False)\n",
    "\n",
    "    df.unpersist() # Uncache the DataFrame after analysis completion\n",
    "    print(\"\\nEDA completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA\n\nSpecify the path to your Delta table and execute. After the analysis, the Spark Session will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Script started\")\n",
    "    # Specify the path to your Delta table. Example: \"./data/mart/user_item_interactions_delta\"\n",
    "    MART_PATH = \"./data/mart/user_item_interactions_delta\" # <--- CHANGE THIS PATH TO YOURS\n",
    "    spark = None # Initialize spark as None to ensure it's defined for the finally block\n",
    "    try:\n",
    "        spark = create_spark_session(\"DataLake_EDA\")\n",
    "        print(\"Spark session created\")\n",
    "        run_eda(spark, MART_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            print(\"Spark session stopped\")\n",
    "        else:\n",
    "            print(\"Spark session was not created or an error occurred before stopping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}